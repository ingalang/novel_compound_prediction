{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "congressional-marketplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import pandas as pd\n",
    "from torch.utils import data\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "import argparse\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import pickle as pkl\n",
    "import warnings\n",
    "pd.options.mode.chained_assignment = None\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"10\"\n",
    "import glob\n",
    "import random\n",
    "random.seed(1991)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import inflect\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "import inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "editorial-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "inflection_engine = inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "gentle-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PARAMETERS\n",
    "constituent = 'mods'\n",
    "dims = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hawaiian-smile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You have a CUDA device, so you should probably run with --cuda\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1612)\n",
    "if not torch.cuda.is_available():\n",
    "    \n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "synthetic-immune",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1.2\n"
     ]
    }
   ],
   "source": [
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "german-grenada",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Steps\n",
    "### 1 Load dev and test data\n",
    "### 2 Use dev data to put together ten datasets, each consisting of dev data + corrupted compounds\n",
    "# 3 Load corrupted modifiers for test as well, and put test + corrupted modifiers together into one dataset\n",
    "# 4 Load word2vec model and get representations for each compound in each set (by concatenation)\n",
    "# (4.2 - Turn representations for each compound into dataframes so they're compatible with Prajit's old code)\n",
    "# 5 Turn sets into tensors etc\n",
    "# 6 Use as inputs to training!!\n",
    "# 7 Then: implement cosine similarity approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "antique-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/COCA_train_min3.txt', 'r') as infile:\n",
    "    train_compounds = [line.strip('\\n\\r') for line in infile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "designing-exercise",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/COCA_dev_min3.txt', 'r') as infile:\n",
    "    dev_compounds = [line.strip('\\n\\r') for line in infile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "medium-cover",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/COCA_test_min3.txt', 'r') as infile:\n",
    "    test_compounds = [line.strip('\\n\\r') for line in infile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "spoken-graphics",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dims == 300:\n",
    "    word2vec_model = Word2Vec.load('word2vec_2009.model')\n",
    "elif dims == 100:\n",
    "    word2vec_model = Word2Vec.load('word2vec_2009_100.model')\n",
    "else: \n",
    "    raise ValueError('300 and 100 dims are the only vector sized that are supported at the moment!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "southern-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corrupted_compounds(constituent, data_name):\n",
    "    assert(data_name in ['dev', 'test']), 'data_name must be either dev or test'\n",
    "    assert(constituent in ['mods', 'heads']), 'constituent must be either mods or heads'\n",
    "    if data_name == 'dev':\n",
    "        corrupted_compound_lists = []\n",
    "        for i in range(10):\n",
    "            with open(f'corrupted_samples/corrupted_{constituent}_10_{i}.txt', 'r') as infile:\n",
    "                corrupted_compounds = [line.strip('\\n\\r') for line in infile]\n",
    "                corrupted_compound_lists.append(corrupted_compounds)\n",
    "        return corrupted_compound_lists\n",
    "    else:\n",
    "        with open(f'corrupted_samples/corrupted_{constituent}_{data_name}.txt', 'r') as infile: \n",
    "            corrupted_compounds = [line.strip('\\n\\r') for line in infile]\n",
    "        return corrupted_compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "israeli-winning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_datasets(dev_compounds, corrupted_samples):\n",
    "    datasets = []\n",
    "    for corrupted_compound_list in corrupted_samples:\n",
    "        datasets.append((dev_compounds, corrupted_compound_list))\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "breathing-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compound_representation(compound, model):\n",
    "    mod, head = compound.split()\n",
    "    mod_vector = model.wv[mod] if mod in model.wv else np.zeros(dims)\n",
    "    head_vector = model.wv[head] if head in model.wv else np.zeros(dims)\n",
    "    assert len(mod_vector) == len(head_vector) == dims, 'length of vectors is wrong fsr'\n",
    "    #if mod not in model.wv:\n",
    "    #    print(f'modifier \\'{mod}\\' not found in word2vec model.')\n",
    "    #if head not in model.wv:\n",
    "    #    print(f'head \\'{head}\\' not found in word2vec model.')\n",
    "    constituents_found = (mod in model.wv, head in model.wv)\n",
    "    return np.concatenate((mod_vector, head_vector)), constituents_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "according-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting together train datasets (from dev data)\n",
    "train_corrupted_compounds = load_corrupted_compounds(constituent, 'dev')\n",
    "train_datasets = generate_train_datasets(dev_compounds, train_corrupted_compounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dirty-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting corrupted samples for test data\n",
    "corrupted_compounds_test = load_corrupted_compounds(constituent, 'test')\n",
    "\n",
    "# making a tuple of positive and negative samples for test data\n",
    "test_dataset = (test_compounds, corrupted_compounds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "medium-guidance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataset_to_tensors(data_tuple, shuffle=True):\n",
    "    # TODO find out what to do with compounds that lack a representation (either partially or completely)\n",
    "    representations_positive = []\n",
    "    representations_negative = []\n",
    "    \n",
    "    mods_not_found = 0\n",
    "    heads_not_found = 0\n",
    "    comps_not_found = 0\n",
    "    \n",
    "    for compound in data_tuple[0]:\n",
    "        representation, constituents_found = get_compound_representation(compound, word2vec_model)\n",
    "        mod_found, head_found = constituents_found\n",
    "        if not mod_found: \n",
    "            mods_not_found += 1\n",
    "        if not head_found: \n",
    "            heads_not_found += 1\n",
    "        if not mod_found and not head_found:\n",
    "            comps_not_found += 1\n",
    "        representations_positive.append(torch.tensor(representation))\n",
    "    \n",
    "    for compound in data_tuple[1]:\n",
    "        representation, constituents_found = get_compound_representation(compound, word2vec_model)\n",
    "        representations_negative.append(torch.tensor(representation))\n",
    "    positive_tensors = torch.stack(representations_positive)\n",
    "    negative_tensors = torch.stack(representations_negative)\n",
    "    \n",
    "    #print('positive mods not found: ', mods_not_found)\n",
    "    #print('positive heads not found: ', heads_not_found)\n",
    "    #print('positive comps not found: ', comps_not_found)\n",
    "    \n",
    "    positive_Y = torch.ones(positive_tensors.shape[0])\n",
    "    negative_Y = torch.zeros(negative_tensors.shape[0])\n",
    "    \n",
    "    X = torch.cat((positive_tensors, negative_tensors))\n",
    "    Y = torch.cat((positive_Y, negative_Y))\n",
    "    \n",
    "    assert len(X) == len(Y), 'X and Y are not of the same length'\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = torch.randperm(X.shape[0])\n",
    "        X = torch.index_select(X, 0, indices)\n",
    "        Y = torch.index_select(Y, 0, indices)\n",
    "    \n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "capital-usage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_compound_list_to_tensors(compound_list):\n",
    "    tensors = []\n",
    "    for comp in compound_list:\n",
    "        representation, _ = get_compound_representation(comp, word2vec_model)\n",
    "        tensors.append(torch.tensor(representation))\n",
    "    return torch.stack(tensors)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aging-mercury",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cosine_similarity_compounds(compound_list, \n",
    "                                         n, \n",
    "                                         word2vec_model, \n",
    "                                         target_constituent, \n",
    "                                         train_compound_dict, \n",
    "                                         dev_compound_dict):\n",
    "    assert(target_constituent in ['modifier', 'head']), 'target_constituent must be either modifier or head'\n",
    "    \n",
    "    novel_compounds_origin_dict = {}\n",
    "    \n",
    "    def join_constituents(new_word, other_constituent):\n",
    "        if target_constituent == 'modifier':\n",
    "            novel_compound = new_word + \" \" + other_constituent\n",
    "        elif target_constituent == 'head':\n",
    "            novel_compound = other_constituent + \" \" + new_word\n",
    "        else: raise ValueError('target_constituent must be either \\'modifier\\' or \\'head\\' ')\n",
    "        return novel_compound\n",
    "    \n",
    "    all_novel_compounds = {}\n",
    "    for compound in compound_list: \n",
    "        num_compounds_found = 0\n",
    "        mod, head = compound.split()\n",
    "        k = 3\n",
    "        if target_constituent == 'modifier':\n",
    "            word_to_change = mod\n",
    "            other_constituent = head\n",
    "        else: \n",
    "            word_to_change = head\n",
    "            other_constituent = mod\n",
    "        #print('word_to_change, other_constituent: ', word_to_change, other_constituent)\n",
    "        #print(f'getting top {n*k} similar words to {word_to_change} ({target_constituent})')\n",
    "        while num_compounds_found < n:\n",
    "            similar_words = word2vec_model.wv.most_similar(word_to_change, topn=n*k)\n",
    "            #print(f'top similar words: {similar_words}')\n",
    "            current_novel_compounds = [join_constituents(word[0], other_constituent) for word in similar_words \n",
    "                                if join_constituents(word[0], other_constituent) not in train_compound_dict \n",
    "                                and join_constituents(word[0], other_constituent) not in dev_compound_dict\n",
    "                                and join_constituents(word[0], other_constituent) not in all_novel_compounds][:n]\n",
    "            num_compounds_found += len(current_novel_compounds)\n",
    "            #print(f'found {len(current_novel_compounds)} feasible words')\n",
    "            all_novel_compounds.update({comp:0 for comp in current_novel_compounds})\n",
    "            #print('increasing k by one')\n",
    "            k += 1\n",
    "            novel_compounds_origin_dict.update({novel_compound: compound for novel_compound in current_novel_compounds})\n",
    "    return list(all_novel_compounds.keys()), novel_compounds_origin_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "knowing-momentum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['antonin principle', 'souter principle', 'rehnquist principle', 'ginsburg principle', 'retribution principle', 'mars today', 'krypton today', 'oceans today', 'pluto today', 'planets today']\n",
      "100733\n",
      "506353\n"
     ]
    }
   ],
   "source": [
    "train_dict = {comp : 0 for comp in train_compounds}\n",
    "dev_dict = {comp : 0 for comp in dev_compounds}\n",
    "novel_compounds, novel_compound_dict = generate_cosine_similarity_compounds(compound_list = dev_compounds,\n",
    "                                                      n = 5, \n",
    "                                                      word2vec_model=word2vec_model,\n",
    "                                                      target_constituent='modifier',\n",
    "                                                      train_compound_dict=train_dict,\n",
    "                                                      dev_compound_dict=dev_dict)\n",
    "print(novel_compounds[:10])\n",
    "print(len(dev_compounds))\n",
    "print(len(novel_compounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "complete-mattress",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506353\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(len(novel_compound_dict))\n",
    "with open('novel_compound_origin_dict.json', 'w') as outfile:\n",
    "    json.dump(novel_compound_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "protected-hundred",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS:\n",
      "dogs run  :  dog run\n",
      "melting cloud  :  boiling cloud\n",
      "riesling sauce  :  wine sauce\n",
      "kevlar jacket  :  nylon jacket\n",
      "waistband blouse  :  sleeve blouse\n",
      "gold tsunami  :  silver tsunami\n",
      "toes ring  :  toe ring\n",
      "boy food  :  man food\n",
      "cappuccino cherry  :  coffee cherry\n",
      "sequined glove  :  buckskin glove\n",
      "kneelength glove  :  buckskin glove\n",
      "lightemitting lamp  :  fluorescent lamp\n",
      "brain moisturizer  :  body moisturizer\n",
      "healthcare burden  :  health burden\n",
      "misrepresentation campaign  :  misinformation campaign\n",
      "hashish store  :  secondhand store\n",
      "brain sculpting  :  body sculpting\n",
      "jog yoga  :  walk yoga\n",
      "\n",
      "TPS:\n",
      "vaccination law  :  sterilization law\n",
      "loot box  :  treasure box\n",
      "pork burger  :  beef burger\n",
      "infection outbreak  :  disease outbreak\n",
      "authentication method  :  encryption method\n",
      "verification code  :  application code\n",
      "tilapia skin  :  salmon skin\n",
      "horseradish juice  :  lime juice\n"
     ]
    }
   ],
   "source": [
    "#FP: \n",
    "print('FPS:')\n",
    "fps = ['dogs run', 'melting cloud', 'riesling sauce', \n",
    "       'kevlar jacket', 'waistband blouse', 'gold tsunami', \n",
    "       'toes ring', 'boy food', 'cappuccino cherry', \n",
    "       'sequined glove', 'kneelength glove', 'lightemitting lamp', \n",
    "       'brain moisturizer', 'healthcare burden', 'misrepresentation campaign', \n",
    "       'hashish store', 'brain sculpting', 'jog yoga']\n",
    "for comp in fps: \n",
    "    if comp in novel_compound_dict:\n",
    "        print(comp, \" : \", novel_compound_dict[comp])\n",
    "    else: \n",
    "        print(f'{comp} not in dictionary')\n",
    "print()\n",
    "print('TPS:')\n",
    "tps = ['vaccination law', 'loot box', 'pork burger', \n",
    "       'infection outbreak', 'authentication method', 'verification code', \n",
    "       'tilapia skin', 'horseradish juice']\n",
    "for comp in tps: \n",
    "    if comp in novel_compound_dict:\n",
    "        print(comp, \" : \", novel_compound_dict[comp])\n",
    "    else: \n",
    "        print(f'{comp} not in dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "congressional-nirvana",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('novel_compound_origins.txt', 'w') as outfile:\n",
    "    for key, value in novel_compound_dict.items():\n",
    "        outfile.write(key + \" : \" + value + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "inside-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_tuple = convert_dataset_to_tensors(test_dataset, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "geological-tongue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 1.,  ..., 0., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(test_data_tuple[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "sophisticated-theta",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive mods not found:  0\n",
      "positive heads not found:  0\n",
      "positive comps not found:  0\n",
      "positive mods not found:  0\n",
      "positive heads not found:  0\n",
      "positive comps not found:  0\n",
      "positive mods not found:  0\n",
      "positive heads not found:  0\n",
      "positive comps not found:  0\n",
      "positive mods not found:  0\n",
      "positive heads not found:  0\n",
      "positive comps not found:  0\n",
      "positive mods not found:  0\n",
      "positive heads not found:  0\n",
      "positive comps not found:  0\n",
      "positive mods not found:  0\n",
      "positive heads not found:  0\n",
      "positive comps not found:  0\n",
      "positive mods not found:  0\n",
      "positive heads not found:  0\n",
      "positive comps not found:  0\n",
      "positive mods not found:  0\n",
      "positive heads not found:  0\n",
      "positive comps not found:  0\n",
      "positive mods not found:  0\n",
      "positive heads not found:  0\n",
      "positive comps not found:  0\n",
      "positive mods not found:  0\n",
      "positive heads not found:  0\n",
      "positive comps not found:  0\n"
     ]
    }
   ],
   "source": [
    "train_data_tuples = []\n",
    "\n",
    "for data_tuple in train_datasets:\n",
    "    tensors = convert_dataset_to_tensors(data_tuple)\n",
    "    train_data_tuples.append(tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "oriental-finder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "forty-mason",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "input_size = dims*2\n",
    "print(input_size)\n",
    "hidden_size = 300\n",
    "num_classes = 2\n",
    "num_epochs = 100\n",
    "batch_size = 72\n",
    "learning_rate = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "unlimited-munich",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes,bias=False)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)#.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "minor-action",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifier(train_datasets, test_dataset, cosine_sim_novel_comp_tensors):\n",
    "    total_accuracy=[]\n",
    "    novel_comp_predictions=[]\n",
    "    disambiguator_predictions=[]\n",
    "    \n",
    "    for train_dataset in train_datasets:\n",
    "        train_X = train_dataset[0].float().to(device)\n",
    "        print('train_X shape:', train_X.shape)\n",
    "        train_Y = train_dataset[1].long().to(device)\n",
    "\n",
    "        test_X = test_dataset[0].float().to(device)\n",
    "        test_Y = test_dataset[1].long().to(device)\n",
    "        model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        n_examples=train_X.shape[0]\n",
    "        for i in range(num_epochs):\n",
    "\n",
    "            cost = 0.\n",
    "\n",
    "            num_batches = n_examples // batch_size\n",
    "            for k in range(num_batches):\n",
    "                start, end = k * batch_size, (k + 1) * batch_size\n",
    "                outputs = model(train_X[start:end])\n",
    "                loss = criterion(outputs, train_Y[start:end])\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            #if (k+1) % 100 == 0:\n",
    "            #print ('Epoch [{}/{}], Loss: {:.4f}'.format(i+1, num_epochs, loss.item()))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            correct=0\n",
    "            total=0\n",
    "            outputs = model(test_X)\n",
    "            if cosine_sim_novel_comp_tensors is not None:\n",
    "                novel_comp_outputs = model(cosine_sim_novel_comp_tensors.float())\n",
    "                _, novel_predicted = torch.max(novel_comp_outputs.data, 1)\n",
    "                novel_comp_predictions.append(novel_predicted)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            disambiguator_predictions.append(predicted)\n",
    "            total += test_Y.size(0)\n",
    "            correct += (predicted == test_Y).sum().item()\n",
    "        curr_acc=100 * correct / total\n",
    "        print(curr_acc)\n",
    "        total_accuracy.append(curr_acc)\n",
    "    return total_accuracy, disambiguator_predictions, novel_comp_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "spatial-attribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_exists_number_insensitive(compound, control_list):\n",
    "    mod, head = compound.split()\n",
    "            \n",
    "    singular_head = inflection_engine.singular_noun(head)\n",
    "    singular_head = singular_head if singular_head else head\n",
    "    singular_compound = ' '.join((mod, singular_head)) \n",
    "            \n",
    "    plural_head = inflection_engine.plural_noun(head)\n",
    "    plural_head = plural_head if plural_head else head                \n",
    "    plural_compound = ' '.join((mod, plural_head)) \n",
    "    return (singular_compound in control_list or plural_compound in control_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "continental-bulletin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_novel_compounds(cosine_novel_compounds, attested_novel_compounds, all_predictions):\n",
    "    predictions_df = pd.DataFrame([tensr.tolist() for tensr in all_predictions])\n",
    "    comp_predictions_per_round = [list(predictions_df[i]) for i in range(len(all_predictions[0]))]\n",
    "    \n",
    "    final_compound_predictions = [max(set(pred_list), key = pred_list.count) \n",
    "                              for pred_list in comp_predictions_per_round]\n",
    "    \n",
    "    comp_to_pred_dict = {compound: prediction \n",
    "                               for compound, prediction \n",
    "                               in zip(cosine_novel_compounds, final_compound_predictions)}\n",
    "    true_positives = [comp for comp in comp_to_pred_dict.keys() \n",
    "                  if comp_exists_number_insensitive(comp, attested_novel_compounds) and comp_to_pred_dict[comp] == 1]\n",
    "    false_positives = [comp for comp in comp_to_pred_dict.keys() \n",
    "                   if not comp_exists_number_insensitive(comp, attested_novel_compounds) and comp_to_pred_dict[comp] == 1]\n",
    "    true_negatives = [comp for comp in comp_to_pred_dict.keys() \n",
    "                  if not comp_exists_number_insensitive(comp, attested_novel_compounds) and comp_to_pred_dict[comp] == 0]\n",
    "    false_negatives = [comp for comp in comp_to_pred_dict.keys()\n",
    "                  if comp_exists_number_insensitive(comp, attested_novel_compounds) and comp_to_pred_dict[comp] == 0]\n",
    "    \n",
    "    print(f'True positives: {len(true_positives)}')\n",
    "    print(f'False positives: {len(false_positives)}')\n",
    "    print(f'True negatives: {len(true_negatives)}')\n",
    "    print(f'False negatives: {len(false_negatives)}')\n",
    "    majority_vote_acc = (len(true_positives) + len(true_negatives)) / \\\n",
    "    (len(true_positives) + len(true_negatives) + len(false_positives) + len(false_negatives))\n",
    "    print(f'Majority-vote accuracy: {majority_vote_acc}')\n",
    "    print(f'Average accuracy: {round(np.mean(total_accuracy), 2)}')\n",
    "    print(f'Average accuracy SD: {round(np.std(total_accuracy), 2)}')\n",
    "    return true_positives, false_positives, true_negatives, false_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "macro-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_disambiguated_compounds(test_data_compounds, test_Y, all_predictions):\n",
    "    predictions_df = pd.DataFrame([tensr.tolist() for tensr in all_predictions])\n",
    "    comp_predictions_per_round = [list(predictions_df[i]) for i in range(len(all_predictions[0]))]\n",
    "    \n",
    "    final_compound_predictions = [max(set(pred_list), key = pred_list.count) \n",
    "                              for pred_list in comp_predictions_per_round]\n",
    "    assert(len(test_data_compounds) == len(test_Y) == len(final_compound_predictions)), 'test_data_compounds, test_Y and final_compound_predictions must be of the same length'\n",
    "        \n",
    "    true_positives = [test_data_compounds[i] for i in range(len(test_data_compounds)) \n",
    "                      if test_Y[i] == final_compound_predictions[i] == 1]\n",
    "    false_positives = [test_data_compounds[i] for i in range(len(test_data_compounds)) \n",
    "                      if test_Y[i] == 0 and final_compound_predictions[i] == 1]\n",
    "    true_negatives = [test_data_compounds[i] for i in range(len(test_data_compounds)) \n",
    "                      if test_Y[i] == final_compound_predictions[i] == 0]\n",
    "    false_negatives = [test_data_compounds[i] for i in range(len(test_data_compounds)) \n",
    "                      if test_Y[i] == 1 and final_compound_predictions[i] == 0]\n",
    "    \n",
    "    print(f'True positives: {len(true_positives)}')\n",
    "    print(f'False positives: {len(false_positives)}')\n",
    "    print(f'True negatives: {len(true_negatives)}')\n",
    "    print(f'False negatives: {len(false_negatives)}')\n",
    "    majority_vote_acc = (len(true_positives) + len(true_negatives)) / \\\n",
    "    (len(true_positives) + len(true_negatives) + len(false_positives) + len(false_negatives))\n",
    "    print(f'Majority-vote accuracy: {majority_vote_acc}')\n",
    "    print(f'Average accuracy: {round(np.mean(total_accuracy), 2)}')\n",
    "    print(f'Average accuracy SD: {round(np.std(total_accuracy), 2)}')\n",
    "    return true_positives, false_positives, true_negatives, false_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "remarkable-colon",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_novel_tensors = convert_compound_list_to_tensors(novel_compounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "spanish-mattress",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X shape: torch.Size([207136, 600])\n",
      "66.96585065197712\n",
      "train_X shape: torch.Size([207136, 600])\n",
      "66.86376856256862\n",
      "train_X shape: torch.Size([207136, 600])\n",
      "66.87725110267917\n",
      "train_X shape: torch.Size([207136, 600])\n",
      "67.09874997592404\n",
      "train_X shape: torch.Size([207136, 600])\n",
      "66.90229010574164\n",
      "train_X shape: torch.Size([207136, 600])\n",
      "67.09297174444808\n",
      "train_X shape: torch.Size([207136, 600])\n",
      "66.50744428821818\n",
      "train_X shape: torch.Size([207136, 600])\n",
      "66.79057763053989\n",
      "train_X shape: torch.Size([207136, 600])\n",
      "67.06408058706832\n",
      "train_X shape: torch.Size([207136, 600])\n",
      "66.99666788651554\n"
     ]
    }
   ],
   "source": [
    "total_accuracy, disambiguator_predictions, novel_compound_predictions = run_classifier(train_data_tuples, test_data_tuple, cosine_novel_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "hungarian-popularity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66.96585065197712, 66.86376856256862, 66.87725110267917, 67.09874997592404, 66.90229010574164, 67.09297174444808, 66.50744428821818, 66.79057763053989, 67.06408058706832, 66.99666788651554]\n"
     ]
    }
   ],
   "source": [
    "print(total_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "textile-geneva",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51919\n",
      "51919\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "all_test_compounds = test_dataset[0] + test_dataset[1]\n",
    "test_Y = list(np.concatenate((np.ones(len(test_dataset[0])), np.zeros(len(test_dataset[1])))))\n",
    "print(len(all_test_compounds))\n",
    "print(len(test_Y))\n",
    "print(len(disambiguator_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "grateful-wrong",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives: 13361\n",
      "False positives: 4813\n",
      "True negatives: 23153\n",
      "False negatives: 10592\n",
      "Majority-vote accuracy: 0.7032878137098172\n",
      "Average accuracy: 66.92\n",
      "Average accuracy SD: 0.17\n"
     ]
    }
   ],
   "source": [
    "true_positives, false_positives, true_negatives, false_negatives = evaluate_disambiguated_compounds(all_test_compounds, test_Y, disambiguator_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "demonstrated-break",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives: 805\n",
      "False positives: 227806\n",
      "True negatives: 277436\n",
      "False negatives: 306\n",
      "Majority-vote accuracy: 0.5495000523350311\n",
      "Average accuracy: 66.83\n",
      "Average accuracy SD: 0.22\n"
     ]
    }
   ],
   "source": [
    "true_positives, false_positives, true_negatives, false_negatives = evaluate_novel_compounds(novel_compounds, test_compounds, novel_compound_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "absent-substance",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results_300/disambiguated/true_positives_disambiguated.txt', 'w') as outfile:\n",
    "    for compound in true_positives:\n",
    "        outfile.write(compound + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "twenty-reynolds",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results_300/disambiguated/true_negatives_disambiguated.txt', 'w') as outfile:\n",
    "    for compound in true_negatives:\n",
    "        outfile.write(compound + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "royal-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results_300/disambiguated/false_positives_disambiguated.txt', 'w') as outfile:\n",
    "    for compound in false_positives:\n",
    "        outfile.write(compound + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "honey-narrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results_300/disambiguated/false_negatives_disambiguated.txt', 'w') as outfile:\n",
    "    for compound in false_negatives:\n",
    "        outfile.write(compound + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-hayes",
   "metadata": {},
   "source": [
    "([67.0332633525299, 66.76746470463607, 66.5421136770739, 67.06215450990966, 66.80406017065043, 67.06215450990966, 67.021706889578, 66.41306650744428, 66.88110325699648, 66.71353454419383], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "precise-bankruptcy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.83\n"
     ]
    }
   ],
   "source": [
    "print(round(np.mean(total_accuracy[0]), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "forbidden-miniature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22\n"
     ]
    }
   ],
   "source": [
    "print(round(np.std(total_accuracy[0]), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-paraguay",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
