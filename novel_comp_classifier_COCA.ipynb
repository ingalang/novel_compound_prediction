{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "congressional-marketplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import pandas as pd\n",
    "from torch.utils import data\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "import argparse\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import pickle as pkl\n",
    "import warnings\n",
    "pd.options.mode.chained_assignment = None\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"10\"\n",
    "import glob\n",
    "import random\n",
    "random.seed(1991)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import inflect\n",
    "from gensim.models import Word2Vec\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "gentle-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PARAMETERS\n",
    "constituent = 'mods'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "hawaiian-smile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You have a CUDA device, so you should probably run with --cuda\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1612)\n",
    "if not torch.cuda.is_available():\n",
    "    \n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "synthetic-immune",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1.2\n"
     ]
    }
   ],
   "source": [
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "german-grenada",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Steps\n",
    "### 1 Load dev and test data\n",
    "### 2 Use dev data to put together ten datasets, each consisting of dev data + corrupted compounds\n",
    "# 3 Load corrupted modifiers for test as well, and put test + corrupted modifiers together into one dataset\n",
    "# 4 Load word2vec model and get representations for each compound in each set (by concatenation)\n",
    "# (4.2 - Turn representations for each compound into dataframes so they're compatible with Prajit's old code)\n",
    "# 5 Turn sets into tensors etc\n",
    "# 6 Use as inputs to training!!\n",
    "# 7 Then: implement cosine similarity approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "designing-exercise",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/COCA_dev_min3.txt', 'r') as infile:\n",
    "    dev_compounds = [line.strip('\\n\\r') for line in infile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "medium-cover",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/COCA_test_min3.txt', 'r') as infile:\n",
    "    test_compounds = [line.strip('\\n\\r') for line in infile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "spoken-graphics",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec.load('word2vec_2009.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "southern-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corrupted_compounds(constituent, data_name):\n",
    "    assert(data_name in ['dev', 'test']), 'data_name must be either dev or test'\n",
    "    assert(constituent in ['mods', 'heads']), 'constituent must be either mods or heads'\n",
    "    if data_name == 'dev':\n",
    "        corrupted_compound_lists = []\n",
    "        for i in range(10):\n",
    "            with open(f'corrupted_samples/corrupted_{constituent}_10_{i}.txt', 'r') as infile:\n",
    "                corrupted_compounds = [line.strip('\\n\\r') for line in infile]\n",
    "                corrupted_compound_lists.append(corrupted_compounds)\n",
    "        return corrupted_compound_lists\n",
    "    else:\n",
    "        with open(f'corrupted_samples/corrupted_{constituent}_{data_name}.txt', 'r') as infile: \n",
    "            corrupted_compounds = [line.strip('\\n\\r') for line in infile]\n",
    "        return corrupted_compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "israeli-winning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_datasets(dev_compounds, corrupted_samples):\n",
    "    datasets = []\n",
    "    for corrupted_compound_list in corrupted_samples:\n",
    "        datasets.append((dev_compounds, corrupted_compound_list))\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "breathing-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compound_representation(compound, model):\n",
    "    mod, head = compound.split()\n",
    "    mod_vector = model.wv[mod] if mod in model.wv else np.zeros(300)\n",
    "    head_vector = model.wv[head] if head in model.wv else np.zeros(300)\n",
    "    assert len(mod_vector) == len(head_vector) == 300, 'length of vectors is wrong fsr'\n",
    "    #if mod not in model.wv:\n",
    "    #    print(f'modifier \\'{mod}\\' not found in word2vec model.')\n",
    "    #if head not in model.wv:\n",
    "    #    print(f'head \\'{head}\\' not found in word2vec model.')\n",
    "    constituents_found = (mod in model.wv, head in model.wv)\n",
    "    return np.concatenate((mod_vector, head_vector)), constituents_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "according-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting together train datasets (from dev data)\n",
    "train_corrupted_compounds = load_corrupted_compounds(constituent, 'dev')\n",
    "train_datasets = generate_train_datasets(dev_compounds, train_corrupted_compounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dirty-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting corrupted samples for test data\n",
    "corrupted_compounds_test = load_corrupted_compounds(constituent, 'test')\n",
    "\n",
    "# making a tuple of positive and negative samples for test data\n",
    "test_dataset = (test_compounds, corrupted_compounds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "medium-guidance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataset_to_tensors(data_tuple, shuffle=True):\n",
    "    # TODO find out what to do with compounds that lack a representation (either partially or completely)\n",
    "    representations_positive = []\n",
    "    representations_negative = []\n",
    "    \n",
    "    mods_not_found = 0\n",
    "    heads_not_found = 0\n",
    "    comps_not_found = 0\n",
    "    \n",
    "    for compound in data_tuple[0]:\n",
    "        representation, constituents_found = get_compound_representation(compound, word2vec_model)\n",
    "        mod_found, head_found = constituents_found\n",
    "        if not mod_found: \n",
    "            mods_not_found += 1\n",
    "        if not head_found: \n",
    "            heads_not_found += 1\n",
    "        if not mod_found and not head_found:\n",
    "            comps_not_found += 1\n",
    "        representations_positive.append(torch.tensor(representation))\n",
    "    \n",
    "    for compound in data_tuple[1]:\n",
    "        representation, constituents_found = get_compound_representation(compound, word2vec_model)\n",
    "        representations_negative.append(torch.tensor(representation))\n",
    "    positive_tensors = torch.stack(representations_positive)\n",
    "    negative_tensors = torch.stack(representations_negative)\n",
    "    \n",
    "    print('positive mods not found: ', mods_not_found)\n",
    "    print('positive heads not found: ', heads_not_found)\n",
    "    print('positive comps not found: ', comps_not_found)\n",
    "    \n",
    "    positive_Y = torch.ones(positive_tensors.shape[0])\n",
    "    negative_Y = torch.zeros(negative_tensors.shape[0])\n",
    "    \n",
    "    X = torch.cat((positive_tensors, negative_tensors))\n",
    "    Y = torch.cat((positive_Y, negative_Y))\n",
    "    \n",
    "    assert len(X) == len(Y), 'X and Y are not of the same length'\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = torch.randperm(X.shape[0])\n",
    "        X = torch.index_select(X, 0, indices)\n",
    "        Y = torch.index_select(Y, 0, indices)\n",
    "    \n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "inside-interval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive mods not found:  0\n",
      "positive heads not found:  0\n",
      "positive comps not found:  0\n"
     ]
    }
   ],
   "source": [
    "test_data_tuple = convert_dataset_to_tensors(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "geological-tongue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 1.,  ..., 0., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(test_data_tuple[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "sophisticated-theta",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive mods not found:  0\n",
      "positive heads not found:  0\n",
      "positive comps not found:  0\n",
      "positive mods not found:  0\n",
      "positive heads not found:  0\n",
      "positive comps not found:  0\n",
      "positive mods not found:  0\n",
      "positive heads not found:  0\n",
      "positive comps not found:  0\n",
      "positive mods not found:  0\n",
      "positive heads not found:  0\n",
      "positive comps not found:  0\n",
      "positive mods not found:  0\n",
      "positive heads not found:  0\n",
      "positive comps not found:  0\n",
      "positive mods not found:  0\n",
      "positive heads not found:  0\n",
      "positive comps not found:  0\n",
      "positive mods not found:  0\n",
      "positive heads not found:  0\n",
      "positive comps not found:  0\n",
      "positive mods not found:  0\n",
      "positive heads not found:  0\n",
      "positive comps not found:  0\n",
      "positive mods not found:  0\n",
      "positive heads not found:  0\n",
      "positive comps not found:  0\n",
      "positive mods not found:  0\n",
      "positive heads not found:  0\n",
      "positive comps not found:  0\n"
     ]
    }
   ],
   "source": [
    "train_data_tuples = []\n",
    "\n",
    "for data_tuple in train_datasets:\n",
    "    tensors = convert_dataset_to_tensors(data_tuple)\n",
    "    train_data_tuples.append(tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "forty-mason",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 600\n",
    "hidden_size = 300\n",
    "num_classes = 2\n",
    "num_epochs = 100\n",
    "batch_size = 72\n",
    "learning_rate = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "unlimited-munich",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes,bias=False)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)#.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "minor-action",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifier(train_datasets, test_dataset, novel_comp_mods=None, novel_comp_heads=None):\n",
    "    total_accuracy=[]\n",
    "    novel_comp_predictions=[]\n",
    "    \n",
    "    for train_dataset in train_datasets:\n",
    "        train_X = train_dataset[0].float().to(device)\n",
    "        print('train_X shape:', train_X.shape)\n",
    "        train_Y = train_dataset[1].long().to(device)\n",
    "\n",
    "        test_X = test_dataset[0].float().to(device)\n",
    "        test_Y = test_dataset[1].long().to(device)\n",
    "        model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        n_examples=train_X.shape[0]\n",
    "        for i in range(num_epochs):\n",
    "\n",
    "            cost = 0.\n",
    "\n",
    "            num_batches = n_examples // batch_size\n",
    "            for k in range(num_batches):\n",
    "                start, end = k * batch_size, (k + 1) * batch_size\n",
    "                outputs = model(train_X[start:end])\n",
    "                loss = criterion(outputs, train_Y[start:end])\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            #if (k+1) % 100 == 0:\n",
    "            #print ('Epoch [{}/{}], Loss: {:.4f}'.format(i+1, num_epochs, loss.item()))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            correct=0\n",
    "            total=0\n",
    "            outputs = model(test_X)\n",
    "            if novel_comp_mods is not None:\n",
    "                novel_comp_outputs = model(novel_compounds_mods_tensors.float())\n",
    "                _, novel_predicted = torch.max(novel_comp_outputs.data, 1)\n",
    "                novel_comp_predictions.append(novel_predicted)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += test_Y.size(0)\n",
    "            correct += (predicted == test_Y).sum().item()\n",
    "        curr_acc=100 * correct / total\n",
    "        print(curr_acc)\n",
    "        total_accuracy.append(curr_acc)\n",
    "    return total_accuracy, novel_comp_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "spanish-mattress",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X shape: torch.Size([207136, 600])\n",
      "67.0332633525299\n",
      "train_X shape: torch.Size([207136, 600])\n",
      "66.76746470463607\n",
      "train_X shape: torch.Size([207136, 600])\n",
      "66.5421136770739\n",
      "train_X shape: torch.Size([207136, 600])\n",
      "67.06215450990966\n",
      "train_X shape: torch.Size([207136, 600])\n",
      "66.80406017065043\n",
      "train_X shape: torch.Size([207136, 600])\n",
      "67.06215450990966\n",
      "train_X shape: torch.Size([207136, 600])\n",
      "67.021706889578\n",
      "train_X shape: torch.Size([207136, 600])\n",
      "66.41306650744428\n",
      "train_X shape: torch.Size([207136, 600])\n",
      "66.88110325699648\n",
      "train_X shape: torch.Size([207136, 600])\n",
      "66.71353454419383\n"
     ]
    }
   ],
   "source": [
    "total_accuracy = run_classifier(train_data_tuples, test_data_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "hungarian-popularity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([67.0332633525299, 66.76746470463607, 66.5421136770739, 67.06215450990966, 66.80406017065043, 67.06215450990966, 67.021706889578, 66.41306650744428, 66.88110325699648, 66.71353454419383], [])\n"
     ]
    }
   ],
   "source": [
    "print(total_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-concrete",
   "metadata": {},
   "source": [
    "([67.0332633525299, 66.76746470463607, 66.5421136770739, 67.06215450990966, 66.80406017065043, 67.06215450990966, 67.021706889578, 66.41306650744428, 66.88110325699648, 66.71353454419383], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "hearing-titanium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.83006221229223\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(total_accuracy[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "caroline-synthesis",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'sd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-39414003d432>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_accuracy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mTester\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m                 raise AttributeError(\"module {!r} has no attribute \"\n\u001b[0m\u001b[1;32m    215\u001b[0m                                      \"{!r}\".format(__name__, attr))\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'sd'"
     ]
    }
   ],
   "source": [
    "print(np.sd(total_accuracy[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-miller",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
