{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "congressional-marketplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import pandas as pd\n",
    "from torch.utils import data\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "import argparse\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import pickle as pkl\n",
    "import warnings\n",
    "pd.options.mode.chained_assignment = None\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"10\"\n",
    "import glob\n",
    "import random\n",
    "random.seed(1612)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import inflect\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import gensim\n",
    "import json\n",
    "import inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "editorial-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "inflection_engine = inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "gentle-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PARAMETERS\n",
    "constituent = 'mods'\n",
    "dims = 300\n",
    "use_frequency_information = True\n",
    "if use_frequency_information:\n",
    "    dims += 5\n",
    "temporal = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "hawaiian-smile",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1612)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "synthetic-immune",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1.2\n"
     ]
    }
   ],
   "source": [
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "german-grenada",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Steps\n",
    "### 1 Load dev and test data\n",
    "### 2 Use dev data to put together ten datasets, each consisting of dev data + corrupted compounds\n",
    "# 3 Load corrupted modifiers for test as well, and put test + corrupted modifiers together into one dataset\n",
    "# 4 Load word2vec model and get representations for each compound in each set (by concatenation)\n",
    "# (4.2 - Turn representations for each compound into dataframes so they're compatible with Prajit's old code)\n",
    "# 5 Turn sets into tensors etc\n",
    "# 6 Use as inputs to training!!\n",
    "# 7 Then: implement cosine similarity approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "antique-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/COCA_train_min3_no_doubles_filtered_new.txt', 'r') as infile:\n",
    "    train_compounds = [line.strip('\\n\\r') for line in infile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "designing-exercise",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/COCA_dev_min3_no_doubles_filtered_new.txt', 'r') as infile:\n",
    "    dev_compounds = [line.strip('\\n\\r') for line in infile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "medium-cover",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/COCA_test_min3_no_doubles_filtered_new.txt', 'r') as infile:\n",
    "    test_compounds = [line.strip('\\n\\r') for line in infile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-gabriel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-estate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "spoken-graphics",
   "metadata": {},
   "outputs": [],
   "source": [
    "if temporal: \n",
    "    with open('encoded_vecs.json', 'rb') as infile:\n",
    "        embedding_model = json.load(infile)\n",
    "elif use_frequency_information:\n",
    "    with open('vecs_with_freqs.json', 'rb') as infile:\n",
    "        embedding_model = json.load(infile)\n",
    "else: \n",
    "    if dims == 300:\n",
    "        embedding_model = Word2Vec.load('word2vec_2009.model')\n",
    "    elif dims == 100:\n",
    "        embedding_model = Word2Vec.load('word2vec_2009_100.model')\n",
    "    elif dims == 0:\n",
    "        embedding_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    else: \n",
    "        raise ValueError('300 and 100 dims are the only vector sized that are supported at the moment!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "direct-commerce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dims==0 or use_frequency_information:\n",
    "    train_compounds = [compound for compound in train_compounds \n",
    "                   if compound.split()[0] in embedding_model and compound.split()[1] in embedding_model]\n",
    "else: \n",
    "    train_compounds = [compound for compound in train_compounds \n",
    "                   if compound.split()[0] in embedding_model.wv and compound.split()[1] in embedding_model.wv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "purple-mechanism",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98866\n",
      "98866\n"
     ]
    }
   ],
   "source": [
    "print(len(dev_compounds))\n",
    "if dims==0 or use_frequency_information:\n",
    "    dev_compounds = [compound for compound in dev_compounds \n",
    "                   if compound.split()[0] in embedding_model and compound.split()[1] in embedding_model]\n",
    "else:\n",
    "    dev_compounds = [compound for compound in dev_compounds \n",
    "                   if compound.split()[0] in embedding_model.wv and compound.split()[1] in embedding_model.wv]\n",
    "print(len(dev_compounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "formed-generator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21800\n",
      "21799\n"
     ]
    }
   ],
   "source": [
    "print(len(test_compounds))\n",
    "if dims==0 or use_frequency_information:\n",
    "    test_compounds = [compound for compound in test_compounds \n",
    "                   if compound.split()[0] in embedding_model and compound.split()[1] in embedding_model]\n",
    "else:\n",
    "    test_compounds = [compound for compound in test_compounds\n",
    "                   if compound.split()[0] in embedding_model.wv and compound.split()[1] in embedding_model.wv]\n",
    "print(len(test_compounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "southern-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corrupted_compounds(constituent, data_name):\n",
    "    assert(data_name in ['dev', 'test']), 'data_name must be either dev or test'\n",
    "    assert(constituent in ['mods', 'heads']), 'constituent must be either mods or heads'\n",
    "    if data_name == 'dev':\n",
    "        corrupted_compound_lists = []\n",
    "        for i in range(10):\n",
    "            with open(f'corrupted_samples_filtered/corrupted_{constituent}_10_{i}.txt', 'r') as infile:\n",
    "                corrupted_compounds = [line.strip('\\n\\r') for line in infile]\n",
    "                corrupted_compound_lists.append(corrupted_compounds)\n",
    "        return corrupted_compound_lists\n",
    "    else:\n",
    "        with open(f'corrupted_samples_filtered/corrupted_{constituent}_{data_name}.txt', 'r') as infile: \n",
    "            corrupted_compounds = [line.strip('\\n\\r') for line in infile]\n",
    "        return corrupted_compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "israeli-winning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_datasets(dev_compounds, corrupted_samples):\n",
    "    datasets = []\n",
    "    for corrupted_compound_list in corrupted_samples:\n",
    "        datasets.append((dev_compounds, corrupted_compound_list))\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "convertible-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_info(word, freq_data):\n",
    "    if word in list(freq_data['word']):\n",
    "            word_freq = freq_data.loc[freq_data['word'] == word]\n",
    "            word_freq = list(word_freq.iloc[0][-5:])\n",
    "            return word_freq\n",
    "    else:\n",
    "        return [0.00]*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "breathing-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compound_representation(compound, model):\n",
    "    mod, head = compound.split()\n",
    "    if temporal or use_frequency_information:\n",
    "        mod_vector = np.array(model[mod]) if mod in model else np.zeros(dims)\n",
    "        head_vector = np.array(model[head]) if head in model else np.zeros(dims)\n",
    "    else:\n",
    "        mod_vector = model.wv[mod] if mod in model.wv else np.zeros(dims)\n",
    "        head_vector = model.wv[head] if head in model.wv else np.zeros(dims)\n",
    "    assert len(mod_vector) == len(head_vector), f'modifier and head vectors are not of same length: {len(mod_vector)} and {len(head_vector)}'\n",
    "    \n",
    "    # we need to check word membership in the embedding model slightly differently depending on\n",
    "    # which embedding model we have loaded\n",
    "    if temporal or use_frequency_information or dims == 0: \n",
    "        constituents_found = (mod in model, head in model)\n",
    "    else: \n",
    "        constituents_found = (mod in model.wv, head in model.wv)\n",
    "    \n",
    "    return np.concatenate((mod_vector, head_vector)), constituents_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "according-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting together train datasets (from dev data)\n",
    "train_corrupted_compounds = load_corrupted_compounds(constituent, 'dev')\n",
    "train_datasets = generate_train_datasets(dev_compounds, train_corrupted_compounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "dirty-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting corrupted samples for test data\n",
    "corrupted_compounds_test = load_corrupted_compounds(constituent, 'test')\n",
    "\n",
    "# making a tuple of positive and negative samples for test data\n",
    "test_dataset = (test_compounds, corrupted_compounds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "medium-guidance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataset_to_tensors(data_tuple, embedding_model, shuffle=True):\n",
    "    # TODO find out what to do with compounds that lack a representation (either partially or completely)\n",
    "    representations_positive = []\n",
    "    representations_negative = []\n",
    "    \n",
    "    mods_not_found = 0\n",
    "    heads_not_found = 0\n",
    "    comps_not_found = 0\n",
    "    \n",
    "    for compound in data_tuple[0]:\n",
    "        representation, constituents_found = get_compound_representation(compound, embedding_model)\n",
    "        mod_found, head_found = constituents_found\n",
    "        if not mod_found: \n",
    "            mods_not_found += 1\n",
    "        if not head_found: \n",
    "            heads_not_found += 1\n",
    "        if not mod_found and not head_found:\n",
    "            comps_not_found += 1\n",
    "        representations_positive.append(torch.tensor(representation))\n",
    "    \n",
    "    for compound in data_tuple[1]:\n",
    "        representation, constituents_found = get_compound_representation(compound, embedding_model)\n",
    "        representations_negative.append(torch.tensor(representation))\n",
    "    positive_tensors = torch.stack(representations_positive)\n",
    "    negative_tensors = torch.stack(representations_negative)\n",
    "    \n",
    "    #print('positive mods not found: ', mods_not_found)\n",
    "    #print('positive heads not found: ', heads_not_found)\n",
    "    #print('positive comps not found: ', comps_not_found)\n",
    "    \n",
    "    positive_Y = torch.ones(positive_tensors.shape[0])\n",
    "    negative_Y = torch.zeros(negative_tensors.shape[0])\n",
    "    \n",
    "    X = torch.cat((positive_tensors, negative_tensors))\n",
    "    Y = torch.cat((positive_Y, negative_Y))\n",
    "    \n",
    "    assert len(X) == len(Y), 'X and Y are not of the same length'\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = torch.randperm(X.shape[0])\n",
    "        X = torch.index_select(X, 0, indices)\n",
    "        Y = torch.index_select(Y, 0, indices)\n",
    "    \n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "capital-usage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_compound_list_to_tensors(compound_list):\n",
    "    tensors = []\n",
    "    for comp in compound_list:\n",
    "        representation, _ = get_compound_representation(comp, embedding_model)\n",
    "        tensors.append(torch.tensor(representation))\n",
    "    return torch.stack(tensors)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "aging-mercury",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cosine_similarity_compounds(compound_list, \n",
    "                                         n, \n",
    "                                         embedding_model, \n",
    "                                         target_constituent, \n",
    "                                         train_compound_dict, \n",
    "                                         dev_compound_dict, dims):\n",
    "    assert(target_constituent in ['mods', 'heads']), 'target_constituent must be either modifier or head'\n",
    "    \n",
    "    print('Generating cosine similarity compounds')\n",
    "    print(len(compound_list))\n",
    "    \n",
    "    novel_compounds_origin_dict = {}\n",
    "    \n",
    "    def join_constituents(new_word, other_constituent):\n",
    "        if target_constituent == 'mods':\n",
    "            novel_compound = new_word + \" \" + other_constituent\n",
    "        elif target_constituent == 'heads':\n",
    "            novel_compound = other_constituent + \" \" + new_word\n",
    "        else: raise ValueError('target_constituent must be either \\'mods\\' or \\'heads\\' ')\n",
    "        return novel_compound\n",
    "    \n",
    "    def is_equivalent(word, word_to_compare):\n",
    "        singular_word = inflection_engine.singular_noun(word)\n",
    "        singular_word = singular_word if singular_word else word\n",
    "        \n",
    "        singular_comparison_word = inflection_engine.singular_noun(word_to_compare)\n",
    "        singular_comparison_word = singular_comparison_word if singular_comparison_word else word_to_compare\n",
    "        \n",
    "        if singular_word == singular_comparison_word:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    all_novel_compounds = {}\n",
    "    for compound in compound_list: \n",
    "        num_compounds_found = 0\n",
    "        mod, head = compound.split()\n",
    "        k = 4\n",
    "        if target_constituent == 'mods':\n",
    "            word_to_change = mod\n",
    "            other_constituent = head\n",
    "        else: \n",
    "            word_to_change = head\n",
    "            other_constituent = mod\n",
    "        #print('word_to_change, other_constituent: ', word_to_change, other_constituent)\n",
    "        #print(f'getting top {n*k} similar words to {word_to_change} ({target_constituent})')\n",
    "        if dims==0:\n",
    "            print('dims == 0')\n",
    "            similar_words = embedding_model.most_similar(word_to_change, topn=n*k)\n",
    "        else:\n",
    "            similar_words = embedding_model.wv.most_similar(word_to_change, topn=n*k)\n",
    "            #print(f'top similar words: {similar_words}')\n",
    "        for word in similar_words:\n",
    "                \n",
    "            current_novel_compounds=[]\n",
    "            new_comp = join_constituents(word[0], other_constituent).lower()\n",
    "            if new_comp not in train_compound_dict \\\n",
    "                and new_comp not in dev_compound_dict \\\n",
    "                and new_comp not in novel_compounds_origin_dict \\\n",
    "                and not is_equivalent(word[0], word_to_change) \\\n",
    "                and not '_' in new_comp:\n",
    "                    \n",
    "                current_novel_compounds.append(new_comp)\n",
    "                #print(f'appended novel compound: {new_comp}')\n",
    "                num_compounds_found += 1\n",
    "                novel_compounds_origin_dict.update({new_comp: compound})\n",
    "            if num_compounds_found >= n:\n",
    "                break\n",
    "                #print(f'found {len(current_novel_compounds)} feasible words')\n",
    "            \n",
    "        if num_compounds_found < n:\n",
    "            print('increasing k by one')\n",
    "            k += 1\n",
    "        if k == 10:\n",
    "            print('K too large, moving on to next compound')\n",
    "            continue\n",
    "            \n",
    "    return list(all_novel_compounds.keys()), novel_compounds_origin_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "knowing-momentum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching already existing file\n"
     ]
    }
   ],
   "source": [
    "novel_comp_dict_filename = f'novel_compound_origin_dict_{constituent}_300.json'\n",
    "if os.path.exists(novel_comp_dict_filename):\n",
    "    print('fetching already existing file')\n",
    "    with open(novel_comp_dict_filename, 'rb') as infile:\n",
    "        novel_compound_dict = json.load(infile)\n",
    "else:\n",
    "    train_dict = {comp : 0 for comp in train_compounds}\n",
    "    dev_dict = {comp : 0 for comp in dev_compounds}\n",
    "    novel_compounds, novel_compound_dict = generate_cosine_similarity_compounds(compound_list = dev_compounds,\n",
    "                                                          n = 3, \n",
    "                                                          embedding_model=embedding_model,\n",
    "                                                          target_constituent=constituent,\n",
    "                                                          train_compound_dict=train_dict,\n",
    "                                                          dev_compound_dict=dev_dict,\n",
    "                                                            dims=dims)\n",
    "    with open(f'novel_compound_origin_dict_{constituent}_{dims}.json', 'w') as outfile:\n",
    "        json.dump(novel_compound_dict, outfile)\n",
    "    print(novel_compounds[:10])\n",
    "    print(len(dev_compounds))\n",
    "    print(len(novel_compounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "complete-mattress",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294577\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(novel_compound_dict))\n",
    "novel_compounds = [comp for comp in novel_compound_dict.keys()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "protected-hundred",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS:\n",
      "dogs run not in dictionary\n",
      "melting cloud not in dictionary\n",
      "riesling sauce not in dictionary\n",
      "kevlar jacket  :  nylon jacket\n",
      "waistband blouse  :  sleeve blouse\n",
      "gold tsunami  :  silver tsunami\n",
      "toes ring not in dictionary\n",
      "boy food  :  guy food\n",
      "cappuccino cherry  :  coffee cherry\n",
      "sequined glove  :  buckskin glove\n",
      "kneelength glove  :  buckskin glove\n",
      "lightemitting lamp  :  fluorescent lamp\n",
      "brain moisturizer  :  body moisturizer\n",
      "healthcare burden  :  health burden\n",
      "misrepresentation campaign  :  disinformation campaign\n",
      "hashish store not in dictionary\n",
      "brain sculpting  :  body sculpting\n",
      "jog yoga not in dictionary\n",
      "\n",
      "TPS:\n",
      "vaccination law  :  sterilization law\n",
      "loot box  :  treasure box\n",
      "pork burger  :  beef burger\n",
      "infection outbreak  :  disease outbreak\n",
      "authentication method  :  encryption method\n",
      "verification code  :  application code\n",
      "tilapia skin  :  salmon skin\n",
      "horseradish juice not in dictionary\n"
     ]
    }
   ],
   "source": [
    "#FP: \n",
    "print('FPS:')\n",
    "fps = ['dogs run', 'melting cloud', 'riesling sauce', \n",
    "       'kevlar jacket', 'waistband blouse', 'gold tsunami', \n",
    "       'toes ring', 'boy food', 'cappuccino cherry', \n",
    "       'sequined glove', 'kneelength glove', 'lightemitting lamp', \n",
    "       'brain moisturizer', 'healthcare burden', 'misrepresentation campaign', \n",
    "       'hashish store', 'brain sculpting', 'jog yoga']\n",
    "for comp in fps: \n",
    "    if comp in novel_compound_dict:\n",
    "        print(comp, \" : \", novel_compound_dict[comp])\n",
    "    else: \n",
    "        print(f'{comp} not in dictionary')\n",
    "print()\n",
    "print('TPS:')\n",
    "tps = ['vaccination law', 'loot box', 'pork burger', \n",
    "       'infection outbreak', 'authentication method', 'verification code', \n",
    "       'tilapia skin', 'horseradish juice']\n",
    "for comp in tps: \n",
    "    if comp in novel_compound_dict:\n",
    "        print(comp, \" : \", novel_compound_dict[comp])\n",
    "    else: \n",
    "        print(f'{comp} not in dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "congressional-nirvana",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(f'novel_compound_origins_{constituent}_{dims}.txt', 'w') as outfile:\n",
    "#    for key, value in novel_compound_dict.items():\n",
    "#        outfile.write(key + \" : \" + value + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "twelve-electronics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('mushroom' in embedding_model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "inside-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_tuple = convert_dataset_to_tensors(test_dataset, embedding_model, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "geological-tongue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data_tuple[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "sophisticated-theta",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tuples = []\n",
    "\n",
    "for data_tuple in train_datasets:\n",
    "    tensors = convert_dataset_to_tensors(data_tuple, embedding_model)\n",
    "    train_data_tuples.append(tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "oriental-finder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "forty-mason",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610\n"
     ]
    }
   ],
   "source": [
    "input_size = dims*2\n",
    "print(input_size)\n",
    "hidden_size = 300\n",
    "num_classes = 2\n",
    "num_epochs = 50\n",
    "batch_size = 72\n",
    "learning_rate = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "unlimited-munich",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes,bias=False)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)#.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "minor-action",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifier(train_datasets, test_dataset, cosine_sim_novel_comp_tensors):\n",
    "    total_accuracy=[]\n",
    "    novel_comp_predictions=[]\n",
    "    disambiguator_predictions=[]\n",
    "    \n",
    "    for train_dataset in train_datasets: \n",
    "        train_X = train_dataset[0].float().to(device)\n",
    "        print('train_X shape:', train_X.shape)\n",
    "        train_Y = train_dataset[1].long().to(device)\n",
    "\n",
    "        test_X = test_dataset[0].float().to(device)\n",
    "        test_Y = test_dataset[1].long().to(device)\n",
    "        model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        n_examples=train_X.shape[0]\n",
    "        for i in range(num_epochs):\n",
    "\n",
    "            cost = 0.\n",
    "\n",
    "            num_batches = n_examples // batch_size\n",
    "            for k in range(num_batches):\n",
    "                start, end = k * batch_size, (k + 1) * batch_size\n",
    "                outputs = model(train_X[start:end])\n",
    "                loss = criterion(outputs, train_Y[start:end])\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            #if (k+1) % 100 == 0:\n",
    "            #print ('Epoch [{}/{}], Loss: {:.4f}'.format(i+1, num_epochs, loss.item()))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            correct=0\n",
    "            total=0\n",
    "            outputs = model(test_X)\n",
    "            if cosine_sim_novel_comp_tensors is not None:\n",
    "                novel_comp_outputs = model(cosine_sim_novel_comp_tensors.float())\n",
    "                _, novel_predicted = torch.max(novel_comp_outputs.data, 1)\n",
    "                novel_comp_predictions.append(novel_predicted)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            disambiguator_predictions.append(predicted)\n",
    "            total += test_Y.size(0)\n",
    "            correct += (predicted == test_Y).sum().item()\n",
    "        curr_acc=100 * correct / total\n",
    "        print(curr_acc)\n",
    "        total_accuracy.append(curr_acc)\n",
    "    return total_accuracy, disambiguator_predictions, novel_comp_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "spatial-attribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_exists_number_insensitive(compound, control_list):\n",
    "    mod, head = compound.split()\n",
    "            \n",
    "    singular_head = inflection_engine.singular_noun(head)\n",
    "    singular_head = singular_head if singular_head else head\n",
    "    singular_compound = ' '.join((mod, singular_head)) \n",
    "            \n",
    "    plural_head = inflection_engine.plural_noun(head)\n",
    "    plural_head = plural_head if plural_head else head                \n",
    "    plural_compound = ' '.join((mod, plural_head)) \n",
    "    return (singular_compound in control_list or plural_compound in control_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "continental-bulletin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_novel_compounds(cosine_novel_compounds, attested_novel_compounds, all_predictions):\n",
    "    print('EVALUATING COSINE GENERATED COMPOUNDS')\n",
    "    predictions_df = pd.DataFrame([tensr.tolist() for tensr in all_predictions])\n",
    "    comp_predictions_per_round = [list(predictions_df[i]) for i in range(len(all_predictions[0]))]\n",
    "    \n",
    "    final_compound_predictions = [max(set(pred_list), key = pred_list.count) \n",
    "                              for pred_list in comp_predictions_per_round]\n",
    "    \n",
    "    comp_to_pred_dict = {compound: prediction \n",
    "                               for compound, prediction \n",
    "                               in zip(cosine_novel_compounds, final_compound_predictions)}\n",
    "    true_positives = [comp for comp in comp_to_pred_dict.keys() \n",
    "                  if comp_exists_number_insensitive(comp, attested_novel_compounds) and comp_to_pred_dict[comp] == 1]\n",
    "    false_positives = [comp for comp in comp_to_pred_dict.keys() \n",
    "                   if not comp_exists_number_insensitive(comp, attested_novel_compounds) and comp_to_pred_dict[comp] == 1]\n",
    "    true_negatives = [comp for comp in comp_to_pred_dict.keys() \n",
    "                  if not comp_exists_number_insensitive(comp, attested_novel_compounds) and comp_to_pred_dict[comp] == 0]\n",
    "    false_negatives = [comp for comp in comp_to_pred_dict.keys()\n",
    "                  if comp_exists_number_insensitive(comp, attested_novel_compounds) and comp_to_pred_dict[comp] == 0]\n",
    "    \n",
    "    print(f'True positives: {len(true_positives)}')\n",
    "    print(f'False positives: {len(false_positives)}')\n",
    "    print(f'True negatives: {len(true_negatives)}')\n",
    "    print(f'False negatives: {len(false_negatives)}')\n",
    "    majority_vote_acc = (len(true_positives) + len(true_negatives)) / \\\n",
    "    (len(true_positives) + len(true_negatives) + len(false_positives) + len(false_negatives))\n",
    "    print(f'Majority-vote accuracy: {majority_vote_acc}')\n",
    "    print(f'Average accuracy: {round(np.mean(total_accuracy), 2)}')\n",
    "    print(f'Average accuracy SD: {round(np.std(total_accuracy), 2)}')\n",
    "    return true_positives, false_positives, true_negatives, false_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "macro-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_disambiguated_compounds(test_data_compounds, test_Y, all_predictions):\n",
    "    print('EVALUATING DISAMBIGUATED COMPOUNDS')\n",
    "    predictions_df = pd.DataFrame([tensr.tolist() for tensr in all_predictions])\n",
    "    comp_predictions_per_round = [list(predictions_df[i]) for i in range(len(all_predictions[0]))]\n",
    "    \n",
    "    # getting majority-vote results\n",
    "    final_compound_predictions = [max(set(pred_list), key = pred_list.count) \n",
    "                              for pred_list in comp_predictions_per_round]\n",
    "    assert(len(test_data_compounds) == len(test_Y) == len(final_compound_predictions)), 'test_data_compounds, test_Y and final_compound_predictions must be of the same length'\n",
    "        \n",
    "    true_positives = [test_data_compounds[i] for i in range(len(test_data_compounds)) \n",
    "                      if test_Y[i] == final_compound_predictions[i] == 1]\n",
    "    false_positives = [test_data_compounds[i] for i in range(len(test_data_compounds)) \n",
    "                      if test_Y[i] == 0 and final_compound_predictions[i] == 1]\n",
    "    true_negatives = [test_data_compounds[i] for i in range(len(test_data_compounds)) \n",
    "                      if test_Y[i] == final_compound_predictions[i] == 0]\n",
    "    false_negatives = [test_data_compounds[i] for i in range(len(test_data_compounds)) \n",
    "                      if test_Y[i] == 1 and final_compound_predictions[i] == 0]\n",
    "    \n",
    "    print(f'True positives: {len(true_positives)}')\n",
    "    print(f'False positives: {len(false_positives)}')\n",
    "    print(f'True negatives: {len(true_negatives)}')\n",
    "    print(f'False negatives: {len(false_negatives)}')\n",
    "    majority_vote_acc = (len(true_positives) + len(true_negatives)) / \\\n",
    "    (len(true_positives) + len(true_negatives) + len(false_positives) + len(false_negatives))\n",
    "    print(f'Majority-vote accuracy: {majority_vote_acc}')\n",
    "    print(f'Average accuracy: {round(np.mean(total_accuracy), 2)}')\n",
    "    print(f'Average accuracy SD: {round(np.std(total_accuracy), 2)}')\n",
    "    return true_positives, false_positives, true_negatives, false_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "remarkable-colon",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_novel_tensors = convert_compound_list_to_tensors(novel_compounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "spanish-mattress",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X shape: torch.Size([197732, 610])\n",
      "71.51540172939747\n",
      "train_X shape: torch.Size([197732, 610])\n",
      "72.02229408931397\n",
      "train_X shape: torch.Size([197732, 610])\n",
      "71.85944631757609\n",
      "train_X shape: torch.Size([197732, 610])\n",
      "71.88467625404252\n",
      "train_X shape: torch.Size([197732, 610])\n",
      "71.84339090346108\n",
      "train_X shape: torch.Size([197732, 610])\n",
      "71.63696415055391\n",
      "train_X shape: torch.Size([197732, 610])\n",
      "71.6805431317232\n",
      "train_X shape: torch.Size([197732, 610])\n",
      "71.68283676231106\n",
      "train_X shape: torch.Size([197732, 610])\n",
      "71.85944631757609\n",
      "train_X shape: torch.Size([197732, 610])\n",
      "71.29980045413886\n"
     ]
    }
   ],
   "source": [
    "total_accuracy, disambiguator_predictions, novel_compound_predictions = run_classifier(train_data_tuples, test_data_tuple, cosine_novel_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "hungarian-popularity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71.51540172939747, 72.02229408931397, 71.85944631757609, 71.88467625404252, 71.84339090346108, 71.63696415055391, 71.6805431317232, 71.68283676231106, 71.85944631757609, 71.29980045413886]\n"
     ]
    }
   ],
   "source": [
    "print(total_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "textile-geneva",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43599\n",
      "43599\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "all_test_compounds = test_dataset[0] + test_dataset[1]\n",
    "test_Y = list(np.concatenate((np.ones(len(test_dataset[0])), np.zeros(len(test_dataset[1])))))\n",
    "print(len(all_test_compounds))\n",
    "print(len(test_Y))\n",
    "print(len(disambiguator_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "grateful-wrong",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATING DISAMBIGUATED COMPOUNDS\n",
      "True positives: 14108\n",
      "False positives: 3332\n",
      "True negatives: 18468\n",
      "False negatives: 7691\n",
      "Majority-vote accuracy: 0.7471731003004656\n",
      "Average accuracy: 71.73\n",
      "Average accuracy SD: 0.2\n"
     ]
    }
   ],
   "source": [
    "true_positives, false_positives, true_negatives, false_negatives = evaluate_disambiguated_compounds(all_test_compounds, test_Y, disambiguator_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "contained-toyota",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'results_{dims}/disambiguated/true_positives_disambiguated_{constituent}_{hidden_size}hidden_{num_epochs}e_temp{temporal}.txt', 'w') as outfile:\n",
    "    for compound in true_positives:\n",
    "        outfile.write(compound + \"\\n\")\n",
    "with open(f'results_{dims}/disambiguated/true_negatives_disambiguated_{constituent}_{hidden_size}hidden_{num_epochs}e_temp{temporal}.txt', 'w') as outfile:\n",
    "    for compound in true_negatives:\n",
    "        outfile.write(compound + \"\\n\")\n",
    "with open(f'results_{dims}/disambiguated/false_positives_disambiguated_{constituent}_{hidden_size}hidden_{num_epochs}e_temp{temporal}.txt', 'w') as outfile:\n",
    "    for compound in false_positives:\n",
    "        outfile.write(compound + \"\\n\")\n",
    "with open(f'results_{dims}/disambiguated/false_negatives_disambiguated_{constituent}_{hidden_size}hidden_{num_epochs}e_temp{temporal}.txt', 'w') as outfile:\n",
    "    for compound in false_negatives:\n",
    "        outfile.write(compound + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "demonstrated-break",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATING COSINE GENERATED COMPOUNDS\n",
      "True positives: 448\n",
      "False positives: 76578\n",
      "True negatives: 217350\n",
      "False negatives: 201\n",
      "Majority-vote accuracy: 0.7393584699416451\n",
      "Average accuracy: 71.73\n",
      "Average accuracy SD: 0.2\n"
     ]
    }
   ],
   "source": [
    "test_dict = {comp for comp in test_compounds}\n",
    "\n",
    "true_positives_cos, false_positives_cos, true_negatives_cos, false_negatives_cos = evaluate_novel_compounds(novel_compounds, test_dict, novel_compound_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "absent-substance",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'results_{dims}/cosine_novel/true_positives_cosine_{constituent}_{hidden_size}hidden_{num_epochs}e_temp{temporal}.txt', 'w') as outfile:\n",
    "    for compound in true_positives_cos:\n",
    "        outfile.write(compound + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "twenty-reynolds",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'results_{dims}/cosine_novel/true_negatives_cosine_{constituent}_{hidden_size}hidden_{num_epochs}e_temp{temporal}.txt', 'w') as outfile:\n",
    "    for compound in true_negatives_cos:\n",
    "        outfile.write(compound + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "royal-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'results_{dims}/cosine_novel/false_positives_cosine_{constituent}_{hidden_size}hidden_{num_epochs}e_temp{temporal}.txt', 'w') as outfile:\n",
    "    for compound in false_positives_cos:\n",
    "        outfile.write(compound + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "honey-narrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'results_{dims}/cosine_novel/false_negatives_cosine_{constituent}_{hidden_size}hidden_{num_epochs}e_temp{temporal}.txt', 'w') as outfile:\n",
    "    for compound in false_negatives_cos:\n",
    "        outfile.write(compound + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-hayes",
   "metadata": {},
   "source": [
    "([67.0332633525299, 66.76746470463607, 66.5421136770739, 67.06215450990966, 66.80406017065043, 67.06215450990966, 67.021706889578, 66.41306650744428, 66.88110325699648, 66.71353454419383], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "precise-bankruptcy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.73\n"
     ]
    }
   ],
   "source": [
    "print(round(np.mean(total_accuracy), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "forbidden-miniature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    }
   ],
   "source": [
    "print(round(np.std(total_accuracy), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-paraguay",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-weapon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
